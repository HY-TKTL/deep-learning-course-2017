{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Exercise 4\n",
    "\n",
    "**Due Friday December 1, before 12:00 PM (noon)**\n",
    "\n",
    "In this fourth computer exercise we are going to work with text and methodologies you can freely choose from [Keras](https://keras.io/). \n",
    "Due to absence of our teacher, this exercise has not been formulated as completely as the previous exercises. Because of this and the impending project work, PLEASE REFRAIN FROM SPENDING **TOO MUCH** TIME ON THIS EXERCISE. Once the project is announced, focus more or on that. We will take this into account when scoring this exercise. \n",
    "Use the discussion board on Moodle to discuss issues in the exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. Classifying text (6pts)\n",
    "\n",
    "Your task is to solve the classification problem for the dataset we present below. You should solve this exercise in similar fashion as the previous exercise (and you can copy-paste any useful tools from there to use them here, as you see fit):\n",
    "- Figure out the model which is suitable for the given data, complete from the hidden layers to the representation of X and the suitable loss function.\n",
    "- Validate and evaluate the model properly.\n",
    "- Document briefly what you did.\n",
    "\n",
    "The necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.datasets import reuters\n",
    "import glob\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "This time our dataset is a dataset of 11,228 newswires from Reuters, labeled over 46 topics. Each wire is encoded as a sequence of word indexes (same conventions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n",
    "                                                         num_words=None,\n",
    "                                                         skip_top=0,\n",
    "                                                         maxlen=None,\n",
    "                                                         test_split=0.2,\n",
    "                                                         seed=113,\n",
    "                                                         start_char=1,\n",
    "                                                         oov_char=2,\n",
    "                                                         index_from=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data is formatted by printing the dimensionalities of the variables. Refer to the keras-documentation for further info: https://keras.io/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train size\", x_train.shape)\n",
    "print(\"y_train size\", y_train.shape)\n",
    "print(\"x_test size\", x_test.shape)\n",
    "print(\"y_test size\", y_test.shape)\n",
    "print(\"\")\n",
    "print(\"Number of classes:\", np.unique(y_train).shape[0])\n",
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide you with a ready embedding. This is a 100-dimensional GloVe -embedding, which works similarly as Word2Vec. You can use either this or the existing features of the dataset. Note that you have to map the above word_index to the words you can find in the embedding, string by string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = 'embedding/'\n",
    "\n",
    "dl_file='glove.6B.zip'\n",
    "dl_url='https://www.cs.helsinki.fi/u/jgpyykko/'\n",
    "get_file(dl_file, dl_url+dl_file, cache_dir='./', cache_subdir=database_path, extract=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_filename = glob.glob(database_path + '*.txt')[0]\n",
    "embedding_data = \"\"\n",
    "print(embedding_filename)\n",
    "with open(embedding_filename, 'r') as fp:\n",
    "    for line in fp:\n",
    "        embedding_data = embedding_data + str(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
