{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Exercise 3\n",
    "\n",
    "**Due Tuesday November 21, before 12:00 PM (noon)**\n",
    "\n",
    "In this second computer exercise we are going to work with images and convolutional neural networks, or CNNs. The entire exercise will be done using [Keras](https://keras.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1. A simple CNN (2 points)\n",
    "\n",
    "We'll start by showing you step by step how to create a simple CNN in Keras.  At some points you'll have to fill some code yourself. You can refer to the [Keras documentation](https://keras.io/) to find the right commands.\n",
    "\n",
    "First, let's load all the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.layers.convolutional import Conv2D\n",
    "\n",
    "import exer3_dataset\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "A key part of machine learning is always handling and preprocessing the dataset.  In this exercise we've made your life easier by having already prepared a dataset and split it into training and testing parts. \n",
    "\n",
    "Run the following command to download the dataset.  The first time you run this it will take while as it's pulling the data down over the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = exer3_dataset.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data is formatted by printing the dimensionalities of the variables (tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can see we have 60000 samples of 28x28 images in `x_train`. The third dimension of the images is just 1 as there is just a single grayscale value. The test set is formatted in the same way, except we have just 10000 samples.\n",
    "\n",
    "The class labels are stored in `y_train`. Let's print the first 10 values just to see what they are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the correct classes for each image.  These actually refer to different types of clothing.  Let's define the mapping from class indices to human-understandable labels as a Python dictionary. We have 10 classes, i.e., 10 categories of images to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "labels = {\n",
    "  0: 'T-shirt/top',\n",
    "  1: 'Trouser',\n",
    "  2: 'Pullover',\n",
    "  3: 'Dress',\n",
    "  4: 'Coat',\n",
    "  5: 'Sandal',\n",
    "  6: 'Shirt',\n",
    "  7: 'Sneaker',\n",
    "  8: 'Bag',\n",
    "  9: 'Ankle boot'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, according to this the first image is of class 9, which is an \"Ankle boot\". Let's look at the first image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img0=x_train[0,:,:].reshape(28,28)\n",
    "print(img0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty hard to decipher. Let's instead draw it as an image, interpreting each number as a grayscale value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img0, cmap='Greys', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suppose that's an ankle boot...\n",
    "\n",
    "Typically we use so called one-hot encoding for the class labels in neural networks.  That is instead of having a single value which can have one of 10 label values (e.g. 0, ..., 9), we have 10 values which can each be 1 or 0 depending on if that class is present. \n",
    "\n",
    "Then for the output we typically expect something that looks like a probability distribution over these 10 classes, i.e., each neuron has a value between 0 and 1 indicating the probability of that class being present. For example if the tenth (last) neuron is 0.8, then we have 80% probability of the image containing an ankle boot. (The sum over all classes should also be 1.0 in order for it be a probability distribution.)\n",
    "\n",
    "Here we'll call a utility function to transform the class labels into a one-hot encoding format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Old format\", y_train[:5])\n",
    "y_train_cat = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test_cat = np_utils.to_categorical(y_test, num_classes)\n",
    "print(\"One-hot encoding\\n\", y_train_cat[:5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take a look at the output above. For example for the first image, which has label 9, the tenth value is 1, the rest are zero.\n",
    "\n",
    "Let's display the first example image of each class just for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(10):\n",
    "    idx = np.argwhere(y_train==l)[0]\n",
    "  \n",
    "    plt.subplot(2, 5, l+1)\n",
    "\n",
    "    img = x_train[idx,:,:].reshape(28,28)\n",
    "\n",
    "    plt.imshow(img, cmap='Greys', interpolation='none')\n",
    "    plt.title(labels[l])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we normalize the images to be in the range 0.0 to 1.0 instead of 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the network\n",
    "\n",
    "OK, let's create a simple CNN that learns to detect these classes. \n",
    "\n",
    "<span style=\"background-color: yellow\">Below you need to fill in the neural network layers</span>, which are (in order):\n",
    "\n",
    "- One 2D convolutional layer with kernel size 3x3 and 32 output filters/features\n",
    "\n",
    "- ReLU activation\n",
    "\n",
    "- Max pooling (2D) of size 2x2\n",
    "\n",
    "- Fully-connected (dense) layer to 10 output units (for the 10 classes)\n",
    "\n",
    "- Finally softmax activation to get a probability-like output.\n",
    "\n",
    "**Hint:** For the first layer you'll need to specify the shape of the input tensor manually by giving this parameter: `input_shape=(28, 28, 1)`.\n",
    "\n",
    "Before the dense layer we need a `Flatten()` layer. This is a special layer in Keras that transforms the 2D output into 1D. The 2D convolution works with neurons in 2D, but the dense layer works in 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers here\n",
    "\n",
    "# Let's use categorical crossentry and sgd optmizer\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='sgd', \n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now let's train it for 10 epochs. This takes roughly 5 minutes on a CPU.\n",
    "\n",
    "We use a batch size of 128, which means that the weight updates are calculated for 128 inputs at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 10 # one epoch typically takes a minute or two\n",
    "\n",
    "history = model.fit(x_train, \n",
    "                    y_train_cat, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=128,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot how the loss and accuracy have changed over the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'])\n",
    "plt.title('loss')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['acc'])\n",
    "plt.title('accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Next, let's how well the model can generalize to data it hasn't seen before, i.e., the test data. Recall from your basic machine learning that this is really the crucial part: it's trivial to learn to perfectly model the training set (you can just memorize each example), the hard part is to learn something general about the classes. So let's try to predict the labels of the test dataset, and compare to the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test_cat, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get roughly 84% above if you have done exactly the same steps.  The real result can vary a lot on the random initialisation as we run only 10 epochs here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the weights\n",
    "\n",
    "An interesting thing is to visualise the learned weights for the convolutional layer.  We have 32 kernels of size 3x3, we can just plot them as images, mapping the weight values to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights for the first convolutional layer\n",
    "w0=model.get_weights()[0][:,:,0,:]\n",
    "\n",
    "# Normalize to range 0.0 - 1.0\n",
    "w0-=np.min(w0)\n",
    "w0/=np.max(w0)\n",
    "\n",
    "for r in range(4):\n",
    "    for c in range(8):\n",
    "        n=r*8+c\n",
    "        plt.subplot(4, 8, n+1)\n",
    "        plt.imshow(w0[:,:,n], interpolation='none')\n",
    "        plt.axis('off')\n",
    "        plt.gray()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They might be a bit hard to interpret, but it seems they have learned to detect various corners and edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2. Make a better CNN (4 points)\n",
    "\n",
    "Make a network that performs better than the very simple one above. For your convenience we have copied the essential code from the previous exercise to the cells below. If you just did the previous exercise you don't need to rerun the first cell.\n",
    "\n",
    "<span style=\"background-color: yellow\">Your task is to do at least five (5) reparameterizations for the previous exercise's network and compare the results. At least one of them should have a 5% improvement in the test set result (generalization). Each reparameterization should change a different aspect in the network, while the rest of the parameters are the same as in 3.1. Print out all of the plots and results for each setup into the notebook you return, and analyze and discuss the results briefly in the last cell in the bottom.</span>\n",
    "\n",
    "You probably need to make a few more cells below, and copy-paste the model code (at least five times).\n",
    "\n",
    "Example parameters to try to change: \n",
    "\n",
    "- number of layers or neurons\n",
    "- activation functions\n",
    "- epochs\n",
    "- batch sizes\n",
    "- optimizer, see [Keras' documentation on optimizers](https://keras.io/optimizers/)\n",
    "- max-pooling on/off on certain layers\n",
    "\n",
    "Notice that changing the final layer's softmax activation plus the categorical_crossentropy loss requires some consideration. Don't do it unless you have a good plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.layers.convolutional import Conv2D\n",
    "\n",
    "import exer3_dataset\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "(x_train, y_train), (x_test, y_test) = exer3_dataset.load_data()\n",
    "\n",
    "# Normalize\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "y_train_cat = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test_cat = np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "\n",
    "# Add model here\n",
    "\n",
    "\n",
    "# You can also try different optimizers below\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='sgd', \n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(x_train, \n",
    "                    y_train_cat, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=128,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and accuracy in training\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'])\n",
    "plt.title('loss')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['acc'])\n",
    "plt.title('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "scores = model.evaluate(x_test, y_test_cat, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
